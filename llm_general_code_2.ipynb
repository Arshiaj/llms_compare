{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, RobertaForSequenceClassification, RobertaTokenizer, \\\n",
    "                         DataCollatorWithPadding, EarlyStoppingCallback,AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Load the SST-2 dataset\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "print(\"--------------------------------Loaded Dataset-------------------------------------------------------------\")\n",
    "\n",
    "# Load the RoBERTa tokenizer\n",
    "#model_name = \"facebook/opt-125m\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels = 2)\n",
    "print(\"--------------------------------Loaded tokenizer and model-------------------------------------------------------------\")\n",
    "\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True, max_length = 512,padding='max_length')\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"--------------------------------Tokenised Data-------------------------------------------------------------\")\n",
    "\n",
    "# Load the pre-trained RoBERTaForSequenceClassification model\n",
    "#model = RobertaForSequenceClassification.from_pretrained('facebook/opt-125m', num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "\n",
    "train_epochs = 1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # Output directory\n",
    "    #evaluation_strategy='steps',              # Evaluation strategy\n",
    "    #save_total_limit=3,                       # Number of total save checkpoints\n",
    "    learning_rate=5e-6,                       # Learning rate\n",
    "    per_device_train_batch_size=4,            # Batch size per device during training\n",
    "    per_device_eval_batch_size=4,             # Batch size for evaluation\n",
    "    num_train_epochs=train_epochs,                       # Number of training epochs\n",
    "    weight_decay=0.01,                        # Weight decay\n",
    "    #push_to_hub=False,                        # Whether to upload the model to the Hugging Face model hub\n",
    "    logging_dir='./logs',                     # Logging directory\n",
    "    logging_steps=500,                        # Logging steps\n",
    "    #load_best_model_at_end=True,              # Whether to load the best model at the end of training\n",
    "    #metric_for_best_model='accuracy',         # Metric for the best model\n",
    "    #greater_is_better=True,                   # Whether the metric for the best model is higher is better or not\n",
    "    #gradient_accumulation_steps=16,           # Number of updates steps to accumulate before backward pass\n",
    "    #per_device_eval_batch_size=2,\n",
    "    seed=42\n",
    "    #fp16=True,\n",
    "    #fp16_opt_level=\"O2\",\n",
    "    #warmup_steps=0,                           # Number of warmup steps\n",
    "    #dataloader_num_workers=4,\n",
    "    #label_smoothing_factor=0.1,\n",
    "    #disable_tqdm=False\n",
    ")\n",
    "\n",
    "# Load the accuracy metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    " \n",
    "  # Remove ignored index (special tokens)\\n\",\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    results = clf_metrics.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"],\"recall\": results[\"overall_recall\"],\"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
    "print(\"--------------------------------Started training-------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "print(\"--------------------------------Finished training-------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "trainer.save_model(\"results/finetuned-model-nlp-{}_{}_epochs\".format(model_name, train_epochs))\n",
    "\n",
    "\n",
    "print(\"--------------------------------Saved Model-------------------------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------Computing Metrics-------------------------------------------------------------\")\n",
    "\n",
    "def print_metrics(dataset):\n",
    "    predictions, labels, _ = trainer.predict(dataset)\n",
    "    #print(\"predictions:\")\n",
    "    #print(predictions)\n",
    "    #print(\"Lables: \")\n",
    "    #print(labels)\n",
    "    p = predictions, labels\n",
    "    k = compute_metrics(p)\n",
    "    print(k)\n",
    "    \n",
    "print_metrics(tokenized_dataset[\"validation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
